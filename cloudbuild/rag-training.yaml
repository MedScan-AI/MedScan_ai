# Cloud Build configuration for RAG model selection and deployment
# Optimized for cost and efficiency
steps:
  - name: 'python:3.10'
    id: 'install-deps'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        if [ "${_SKIP_EXPERIMENTS}" = "true" ]; then
          echo "SKIP_EXPERIMENTS=true - Skipping dependency installation"
          exit 0
        fi
        echo "Installing RAG dependencies"
        pip install --upgrade pip
        cd ModelDevelopment/RAG
        pip install -r requirements.txt
        echo "Dependencies installed"

  - name: 'gcr.io/cloud-builders/gsutil'
    id: 'verify-gcs-data'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        set -e
        echo "Verifying RAG data in GCS"
        
        BUCKET="gs://${GCS_BUCKET_NAME:-medscan-pipeline-medscanai-476500}"
        ERROR_MSG=""
        
        if ! gsutil ls $$BUCKET/RAG/index/index_latest.bin > /dev/null 2>&1; then
          ERROR_MSG="FAISS index not found at $$BUCKET/RAG/index/index_latest.bin. Run DataPipeline first: docker-compose exec webserver airflow dags trigger rag_data_pipeline_dvc"
          echo "$$ERROR_MSG"
          echo "$$ERROR_MSG" > /workspace/pipeline_error.txt
          exit 1
        fi
        echo "FAISS index found"
        
        if ! gsutil ls $$BUCKET/RAG/index/embeddings_latest.json > /dev/null 2>&1; then
          ERROR_MSG="Embeddings not found at $$BUCKET/RAG/index/embeddings_latest.json"
          echo "$$ERROR_MSG"
          echo "$$ERROR_MSG" > /workspace/pipeline_error.txt
          exit 1
        fi
        echo "Embeddings found"
        
        gsutil ls $$BUCKET/RAG/index/data_latest.pkl > /dev/null 2>&1 && echo "data.pkl found" || echo "data.pkl not found (optional)"
        
        echo "GCS data verification complete"

  - name: 'python:3.10'
    id: 'run-experiments'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        if [ "${_SKIP_EXPERIMENTS}" = "true" ]; then
          echo "SKIP_EXPERIMENTS=true - Skipping experiment run"
          echo "Using existing config from previous run"
          cd ModelDevelopment/RAG
          # Check if config exists, if not create a dummy one for testing
          if [ ! -f "utils/RAG_config.json" ]; then
            echo "Warning: No existing config found. Creating dummy config for testing."
            mkdir -p utils
            echo '{"model_name": "test-model", "performance_metrics": {"composite_score": 0.5}, "embedding_model": "test-embedding"}' > utils/RAG_config.json
          fi
          echo "Skipped experiments - using existing config"
          exit 0
        fi
        
        echo "Starting RAG model selection experiments"
        pip install --upgrade pip
        cd ModelDevelopment/RAG
        pip install -r requirements.txt
        
        export MLFLOW_TRACKING_URI=file:///workspace/mlruns
        export GCP_PROJECT_ID=${GCP_PROJECT_ID:-medscanai-476500}
        export GCS_BUCKET_NAME=${GCS_BUCKET_NAME:-medscan-pipeline-medscanai-476500}
        
        python ModelSelection/experiment.py
        
        echo "Experiments completed"
    timeout: 3000s

  - name: 'python:3.10'
    id: 'select-best-model'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        if [ "${_SKIP_EXPERIMENTS}" = "true" ]; then
          echo "SKIP_EXPERIMENTS=true - Skipping model selection (using existing config)"
          exit 0
        fi
        
        echo "Selecting best model"
        pip install --upgrade pip
        cd ModelDevelopment/RAG
        # Only install MLflow for model selection, not full requirements
        pip install mlflow>=2.10.0 pyyaml>=6.0
        
        export MLFLOW_TRACKING_URI=file:///workspace/mlruns
        
        python ModelSelection/select_best_model.py
        
        if [ ! -f "utils/RAG_config.json" ]; then
          echo "Config file not created"
          exit 1
        fi
        
        echo "Best model selected"
        cat utils/RAG_config.json | python -m json.tool

  - name: 'python:3.10'
    id: 'validate-model'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        echo "Validating model performance"
        # No pip install needed - only uses Python stdlib (json)
        cd ModelDevelopment/RAG
        
        CONFIG_PATH="utils/RAG_config.json"
        VALIDATION_THRESHOLD=0.2
        
        if [ ! -f "$$CONFIG_PATH" ]; then
          echo "Config file not found for validation"
          exit 1
        fi
        
        COMPOSITE_SCORE=$$(python -c "import json; print(json.load(open('$$CONFIG_PATH'))['performance_metrics']['composite_score'])")
        MODEL_NAME=$$(python -c "import json; print(json.load(open('$$CONFIG_PATH'))['model_name'])")
        
        echo "Composite Score: $$COMPOSITE_SCORE"
        echo "Validation Threshold: $$VALIDATION_THRESHOLD"
        
        # Check if score meets threshold
        VALIDATION_PASSED=$$(python -c "score=$$COMPOSITE_SCORE; threshold=$$VALIDATION_THRESHOLD; print('true' if score >= threshold else 'false')")
        
        echo "$$VALIDATION_PASSED" > /workspace/validation_result.txt
        echo "$$COMPOSITE_SCORE" > /workspace/composite_score.txt
        echo "$$VALIDATION_THRESHOLD" > /workspace/validation_threshold.txt
        echo "$$MODEL_NAME" > /workspace/model_name.txt
        
        if [ "$$VALIDATION_PASSED" = "false" ]; then
          echo "VALIDATION FAILED: Composite score ($$COMPOSITE_SCORE) is below threshold ($$VALIDATION_THRESHOLD)"
          
          # Send validation failure email
          SMTP_USER=$$(gcloud secrets versions access latest --secret="smtp-username" --project=${GCP_PROJECT_ID:-medscanai-476500} 2>&1)
          SMTP_PASSWORD=$$(gcloud secrets versions access latest --secret="smtp-password" --project=${GCP_PROJECT_ID:-medscanai-476500} 2>&1)
          
          if echo "$$SMTP_USER" | grep -q "ERROR\|Permission denied\|not found"; then
            SMTP_USER=""
            SMTP_PASSWORD=""
          fi
          
          export SMTP_USER="$$SMTP_USER"
          export SMTP_PASSWORD="$$SMTP_PASSWORD"
          export GCP_PROJECT_ID=${GCP_PROJECT_ID:-medscanai-476500}
          
          python utils/send_notification.py \
            --type validation-failure \
            --build-id ${BUILD_ID} \
            --model-name "$$MODEL_NAME" \
            --composite-score $$COMPOSITE_SCORE \
            --threshold $$VALIDATION_THRESHOLD || echo "Validation failure email failed (non-critical)"
          
          exit 1
        fi
        
        echo "Validation PASSED: Model meets minimum performance threshold"

  - name: 'python:3.10'
    id: 'extract-bias-results'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        echo "Extracting bias check results"
        cd ModelDevelopment/RAG
        # Only install minimal dependencies - MLflow will be installed later if needed
        
        # First try to get bias results from experiment output file
        if [ -f "bias_results.json" ]; then
          echo "Found bias_results.json from experiment"
          cp bias_results.json /workspace/bias_results.json
        else
          # Fallback: Try to extract from MLflow
          echo "No bias_results.json found, trying MLflow"
          export MLFLOW_TRACKING_URI=file:///workspace/mlruns
          
          # Only install MLflow, not full requirements
          pip install --upgrade pip
          pip install mlflow>=2.10.0
          
          # Extract bias results from MLflow - write script and execute
          printf 'import mlflow\nimport json\nimport os\n\ntry:\n    client = mlflow.tracking.MlflowClient()\n    experiment = client.get_experiment_by_name("RAG_Model_Selection")\n    bias_result = {"bias_detected": False, "num_violations": 0, "violations": [], "recommendations": []}\n    \n    if experiment:\n        runs = client.search_runs(experiment_ids=[experiment.experiment_id], order_by=["metrics.composite_score DESC"], max_results=1)\n        if runs:\n            best_run = runs[0]\n            metrics = best_run.data.metrics\n            bias_result["bias_detected"] = bool(metrics.get("bias_detected", False))\n            bias_result["num_violations"] = int(metrics.get("bias_violations_count", 0) or 0)\n            try:\n                artifacts = client.list_artifacts(best_run.info.run_id)\n                for artifact in artifacts:\n                    if "bias_violations" in artifact.path:\n                        violation_file = client.download_artifacts(best_run.info.run_id, artifact.path)\n                        with open(violation_file, "r") as f:\n                            bias_result["violations"] = json.load(f).get("violations", [])\n            except:\n                pass\n    \n    with open("/workspace/bias_results.json", "w") as f:\n        json.dump(bias_result, f, indent=2)\n    detected = bias_result["bias_detected"]\n    violations = bias_result["num_violations"]\n    print("Bias check results extracted: detected=" + str(detected) + ", violations=" + str(violations))\nexcept Exception as e:\n    print("Error extracting bias results: " + str(e))\n    bias_result = {"bias_detected": False, "num_violations": 0, "violations": [], "recommendations": []}\n    with open("/workspace/bias_results.json", "w") as f:\n        json.dump(bias_result, f, indent=2)\n' > /tmp/extract_bias.py
          python /tmp/extract_bias.py
        fi
        
        # Check if bias was detected
        BIAS_DETECTED=$$(python -c "import json; print(json.load(open('/workspace/bias_results.json'))['bias_detected'])")
        echo "$$BIAS_DETECTED" > /workspace/bias_detected.txt
        
        if [ "$$BIAS_DETECTED" = "True" ]; then
          echo "Bias detected in model - will send notification"
        else
          echo "No bias detected"
        fi

  - name: 'python:3.10'
    id: 'deploy-rag'
    waitFor: ['validate-model']
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        echo "Deploying RAG to Vertex AI"
        pip install --upgrade pip
        cd ModelDevelopment/RAG
        # Use minimal requirements for deployment to avoid OOM
        pip install -r requirements-deploy.txt
        
        export GCP_PROJECT_ID=${GCP_PROJECT_ID:-medscanai-476500}
        export GCS_BUCKET_NAME=${GCS_BUCKET_NAME:-medscan-pipeline-medscanai-476500}
        
        BUCKET="gs://$$GCS_BUCKET_NAME"
        
        python deploy.py \
          --config utils/RAG_config.json \
          --index $$BUCKET/RAG/index/index_latest.bin \
          --embeddings $$BUCKET/RAG/index/embeddings_latest.json \
          --metadata utils/RAG_config.json
        
        echo "RAG deployment completed"

  - name: 'gcr.io/cloud-builders/gsutil'
    id: 'upload-experiments'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        BUCKET_NAME=${GCS_BUCKET_NAME:-medscan-pipeline-medscanai-476500}
        if [ -d "/workspace/mlruns" ] && [ "$(ls -A /workspace/mlruns 2>/dev/null)" ]; then
          echo "Uploading MLflow experiments..."
          gsutil -m cp -r /workspace/mlruns/ gs://$$BUCKET_NAME/RAG/experiments/${BUILD_ID}/ || echo "Upload failed but continuing"
        else
          echo "No MLflow experiments found - skipping upload"
        fi

  - name: 'gcr.io/cloud-builders/gsutil'
    id: 'upload-config'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        BUCKET_NAME=${GCS_BUCKET_NAME:-medscan-pipeline-medscanai-476500}
        if [ -f "ModelDevelopment/RAG/utils/RAG_config.json" ]; then
          gsutil cp ModelDevelopment/RAG/utils/RAG_config.json gs://$$BUCKET_NAME/RAG/models/${BUILD_ID}/config.json
          echo "Config uploaded successfully"
        else
          echo "Config file not found - skipping upload"
        fi

  - name: 'python:3.10'
    id: 'send-success-email'
    waitFor: ['deploy-rag', 'extract-bias-results']
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        echo "Sending training completion notification"
        
        pip install --upgrade pip
        cd ModelDevelopment/RAG
        # Only install email dependencies, not full requirements
        pip install secure-smtplib>=0.1.1
        
        CONFIG_PATH="utils/RAG_config.json"
        
        if [ ! -f "$$CONFIG_PATH" ]; then
          echo "Config not found"
          exit 1
        fi
        
        # Get SMTP credentials from Secret Manager
        SMTP_USER=$$(gcloud secrets versions access latest --secret="smtp-username" --project=${GCP_PROJECT_ID:-medscanai-476500} 2>&1)
        SMTP_PASSWORD=$$(gcloud secrets versions access latest --secret="smtp-password" --project=${GCP_PROJECT_ID:-medscanai-476500} 2>&1)
        
        # Check if secrets were retrieved successfully
        if echo "$$SMTP_USER" | grep -q "ERROR\|Permission denied\|not found"; then
          echo "Warning: Could not retrieve SMTP credentials from Secret Manager"
          SMTP_USER=""
          SMTP_PASSWORD=""
        fi
        
        export SMTP_USER="$$SMTP_USER"
        export SMTP_PASSWORD="$$SMTP_PASSWORD"
        export GCP_PROJECT_ID=${GCP_PROJECT_ID:-medscanai-476500}
        export GCS_BUCKET_NAME=${GCS_BUCKET_NAME:-medscan-pipeline-medscanai-476500}
        
        # Read validation and bias results
        VALIDATION_PASSED=$$(cat /workspace/validation_result.txt 2>/dev/null || echo "true")
        COMPOSITE_SCORE=$$(cat /workspace/composite_score.txt 2>/dev/null || python -c "import json; print(json.load(open('$$CONFIG_PATH'))['performance_metrics']['composite_score'])")
        BIAS_DETECTED=$$(cat /workspace/bias_detected.txt 2>/dev/null || echo "False")
        
        MODEL_NAME=$$(python -c "import json; print(json.load(open('$$CONFIG_PATH'))['model_name'])")
        
        # Determine bias check passed (opposite of bias detected)
        if [ "$$BIAS_DETECTED" = "True" ]; then
          BIAS_CHECK_PASSED="false"
          # Send bias failure email
          python utils/send_notification.py \
            --type bias-failure \
            --build-id ${BUILD_ID} \
            --model-name "$$MODEL_NAME" \
            --bias-details /workspace/bias_results.json || echo "Bias failure email failed (non-critical)"
        else
          BIAS_CHECK_PASSED="true"
        fi
        
        # Send completion email
        python utils/send_notification.py \
          --type completion \
          --build-id ${BUILD_ID} \
          --model-name "$$MODEL_NAME" \
          --composite-score $$COMPOSITE_SCORE \
          --validation-passed "$$VALIDATION_PASSED" \
          --bias-check-passed "$$BIAS_CHECK_PASSED" \
          --bias-details /workspace/bias_results.json || echo "Completion email failed (non-critical)"

  - name: 'ubuntu'
    id: 'build-summary'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        BUCKET_NAME=${GCS_BUCKET_NAME:-medscan-pipeline-medscanai-476500}
        echo "Build Summary"
        echo "================"
        echo "Build ID: ${BUILD_ID}"
        echo "Artifacts:"
        echo "  - MLflow: gs://$$BUCKET_NAME/RAG/experiments/${BUILD_ID}/"
        echo "  - Config: gs://$$BUCKET_NAME/RAG/models/${BUILD_ID}/config.json"
        echo "  - Deployment: gs://$$BUCKET_NAME/RAG/deployments/latest/deployment_info.json"

substitutions:
  _SKIP_EXPERIMENTS: 'false'  # Set to 'true' to skip experiment run (saves ~20 minutes)

timeout: 3600s

options:
  machineType: 'E2_HIGHCPU_8'
  diskSizeGb: 50
  logging: CLOUD_LOGGING_ONLY
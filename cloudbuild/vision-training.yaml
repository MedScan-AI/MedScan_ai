# Cloud Build configuration for Vision model training
steps:
  # Step 1: Install dependencies
  - name: 'python:3.10'
    id: 'install-deps'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        echo "Installing dependencies..."
        pip install --upgrade pip
        pip install -r ModelDevelopment/Vision/requirements.txt
        echo "Dependencies installed"

  # Step 2: Install DVC and pull preprocessed data (combined - DVC must be in same step)
  - name: 'python:3.10'
    id: 'pull-dvc-data'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        echo "Installing DVC and git..."
        pip install --upgrade pip
        # Install DVC with GCS support
        pip install dvc dvc-gs
        # Install git for DVC compatibility (some versions require git even with --no-scm)
        apt-get update && apt-get install -y git || echo "Git installation failed (may already be installed)"
        echo "DVC, dvc-gs, and git installed"
        
        # Verify dvc-gs is installed
        python -c "import dvc_gs; print('dvc-gs installed successfully')" || echo "WARNING: dvc-gs import failed"
        
        echo "Configuring DVC remote..."
        cd /workspace/DataPipeline
        
        # Cloud Build runs with service account permissions
        # Check if credentials are available
        if [ -f /workspace/airflow/gcp-service-account.json ]; then
          export GOOGLE_APPLICATION_CREDENTIALS=/workspace/airflow/gcp-service-account.json
          echo "Using service account key from workspace"
        elif [ -n "$$GOOGLE_APPLICATION_CREDENTIALS" ]; then
          echo "Using GOOGLE_APPLICATION_CREDENTIALS from environment"
        else
          echo "Using Cloud Build default service account"
        fi
        
        # Initialize minimal git repo first (some DVC versions require it even with --no-scm)
        echo "Initializing minimal git repository for DVC compatibility..."
        if ! git rev-parse --git-dir >/dev/null 2>&1; then
          git init
          git config user.email "cloud-build@medscan.ai"
          git config user.name "Cloud Build"
          echo "Git repository initialized"
        else
          echo "Git repository already exists"
        fi
        
        # Initialize DVC with --no-scm (git exists but DVC won't use it for version control)
        echo "Initializing DVC in no-scm mode..."
        
        # Remove existing .dvc directory if it exists (from repo)
        # This ensures fresh initialization with --no-scm
        if [ -d .dvc ]; then
          echo "Removing existing .dvc directory for fresh initialization..."
          rm -rf .dvc
        fi
        
        # Initialize DVC with --no-scm flag (git exists but won't be used for DVC tracking)
        dvc init --no-scm
        
        # Verify DVC is properly initialized
        if [ ! -f .dvc/config ]; then
          echo "ERROR: DVC initialization failed"
          exit 1
        fi
        
        echo "DVC initialized successfully in no-scm mode"
        
        # Configure DVC remote
        dvc remote add -d vision gs://medscan-pipeline-medscanai-476500/dvc-storage/vision 2>/dev/null || \
        dvc remote modify vision url gs://medscan-pipeline-medscanai-476500/dvc-storage/vision
        
        # Set project for GCS access
        dvc remote modify vision projectname medscanai-476500 2>/dev/null || true
        
        # Verify remote configuration
        echo "DVC remote configuration:"
        dvc remote list || echo "Remote list failed (non-critical)"
        
        # Verify .dvc file exists (it should be in the uploaded source code)
        echo "Current directory: $$(pwd)"
        echo "Checking workspace structure:"
        ls -la /workspace/ | head -10
        echo ""
        echo "Checking DataPipeline structure:"
        ls -la /workspace/DataPipeline/ 2>/dev/null | head -10 || echo "DataPipeline not found at /workspace/DataPipeline"
        echo ""
        echo "Searching for .dvc files in workspace:"
        find /workspace -name "*.dvc" -type f 2>/dev/null | head -20
        echo ""
        echo "Checking if we're in the right location:"
        if [ -f data/preprocessed.dvc ]; then
          echo "Found data/preprocessed.dvc in current directory"
        else
          echo "data/preprocessed.dvc NOT found in current directory"
          echo "Trying to find it relative to workspace root..."
          # Check if we need to navigate from workspace root
          if [ -f /workspace/DataPipeline/data/preprocessed.dvc ]; then
            echo "Found at /workspace/DataPipeline/data/preprocessed.dvc"
            cd /workspace/DataPipeline
          else
            echo "ERROR: data/preprocessed.dvc file not found anywhere!"
            echo "Available .dvc files:"
            find /workspace -name "*.dvc" -type f 2>/dev/null | head -20
            exit 1
          fi
        fi
        
        echo "Pulling preprocessed data from DVC..."
        # Try pulling with explicit remote and no git check
        # Some DVC versions require git even with --no-scm, so we handle gracefully
        if ! dvc pull data/preprocessed.dvc -r vision --jobs 4 2>&1; then
          echo "DVC pull failed, checking error..."
          echo "If error is about git repository, DVC might need git even with --no-scm"
          echo "Attempting workaround: creating minimal git repo..."
          
          # Create minimal git repo if needed (DVC workaround)
          if ! git rev-parse --git-dir >/dev/null 2>&1; then
            echo "Initializing minimal git repository for DVC compatibility..."
            git init
            git config user.email "cloud-build@medscan.ai"
            git config user.name "Cloud Build"
            # Don't commit anything, just initialize
          fi
          
          # Retry pull after git init
          echo "Retrying DVC pull..."
          dvc pull data/preprocessed.dvc -r vision --jobs 4
          
          if [ $$? -ne 0 ]; then
            echo "ERROR: DVC pull failed even after git initialization"
            exit 1
          fi
        fi
        
        echo "DVC pull completed successfully"
        
        echo "Data pulled successfully"
        echo "Checking downloaded data structure..."
        find data/preprocessed -type d -maxdepth 2 2>/dev/null | head -10
        find data/preprocessed -type f -name "*.jpg" 2>/dev/null | head -5

  # Step 3: Verify DVC data downloaded
  - name: 'ubuntu'
    id: 'verify-data'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        echo "Verifying DVC data..."
        cd /workspace/DataPipeline
        
        # Check if data exists
        if [ ! -d "data/preprocessed" ]; then
          echo "ERROR: data/preprocessed directory not found!"
          exit 1
        fi
        
        echo "Preprocessed data structure:"
        ls -la data/preprocessed/ | head -10
        
        echo "Finding dataset directories..."
        find data/preprocessed -type d -maxdepth 2 | head -10
        
        echo "Counting images..."
        TB_COUNT=$$(find data/preprocessed/tb -name "*.jpg" 2>/dev/null | wc -l)
        LC_COUNT=$$(find data/preprocessed/lung_cancer* -name "*.jpg" 2>/dev/null | wc -l)
        echo "TB images: $$TB_COUNT"
        echo "Lung Cancer images: $$LC_COUNT"
        
        if [ "$$TB_COUNT" -eq 0 ] && [ "$$LC_COUNT" -eq 0 ]; then
          echo "ERROR: No images found in preprocessed data!"
          exit 1
        fi
        
        # Create symlink/copy for training script compatibility
        mkdir -p /workspace/data
        if [ ! -e "/workspace/data/preprocessed" ]; then
          ln -sf /workspace/DataPipeline/data/preprocessed /workspace/data/preprocessed || \
          cp -r /workspace/DataPipeline/data/preprocessed /workspace/data/preprocessed
        fi

  # Step 4: Train ResNet50 model
  - name: 'python:3.10'
    id: 'train-resnet'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        echo "Installing dependencies for training..."
        pip install --upgrade pip
        pip install -r ModelDevelopment/Vision/requirements.txt
        echo "Starting ResNet50 training..."
        cd ModelDevelopment/Vision
        python train_resnet.py \
          --config ../config/vision_training.yml \
          --data_path /workspace/DataPipeline/data/preprocessed \
          --output_path /workspace/models \
          --datasets ${_DATASET} \
          --epochs ${_EPOCHS}
        echo "Training completed"
    env:
      - 'TF_CPP_MIN_LOG_LEVEL=2'
      - 'PYTHONUNBUFFERED=1'

  # Step 5: Validate model performance
  - name: 'python:3.10'
    id: 'validate-model'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        echo "Installing dependencies for validation..."
        pip install --upgrade pip
        pip install -r ModelDevelopment/Vision/requirements.txt
        echo "Validating model..."
        cd ModelDevelopment/Vision
        
        # Find model in date-partitioned structure (models/YYYY/MM/DD/HHMMSS/CNN_ResNet50_best.keras)
        MODEL_PATH=$$(find /workspace/models -name "CNN_ResNet50_best.keras" -type f | head -1)
        
        # Find test data directory
        # Find test data directory (check DataPipeline location first)
        TEST_DATA=$$(find /workspace/DataPipeline/data/preprocessed -type d \( -path "*/${_DATASET}/*/test" -o -path "*/${_DATASET}/test" \) 2>/dev/null | head -1)
        if [ -z "$$TEST_DATA" ]; then
          TEST_DATA=$$(find /workspace/data/preprocessed -type d \( -path "*/${_DATASET}/*/test" -o -path "*/${_DATASET}/test" \) 2>/dev/null | head -1)
        fi
        
        echo "Model: $$MODEL_PATH"
        echo "Test data: $$TEST_DATA"
        
        # Check if model exists
        if [ -z "$$MODEL_PATH" ] || [ ! -f "$$MODEL_PATH" ]; then
          echo "ERROR: Model file not found!"
          find /workspace/models -type f -name "*.keras" 2>/dev/null | head -5
          exit 1
        fi
        
        # Check if test data exists
        if [ -z "$$TEST_DATA" ] || [ ! -d "$$TEST_DATA" ]; then
          echo "ERROR: Test data directory not found!"
          echo "Searching in DataPipeline:"
          find /workspace/DataPipeline/data/preprocessed -type d 2>/dev/null | head -10
          echo "Searching in workspace/data:"
          find /workspace/data/preprocessed -type d 2>/dev/null | head -10
          exit 1
        fi
        
        python validate.py \
          --model_path "$$MODEL_PATH" \
          --test_data_path "$$TEST_DATA" \
          --output_file /workspace/validation_results.json
        
        if [ $? -ne 0 ]; then
          echo "Validation FAILED - stopping deployment"
          exit 1
        fi
        
        echo "Validation PASSED"

  # Step 6: Bias detection
  - name: 'python:3.10'
    id: 'bias-check'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        echo "Installing dependencies for bias check..."
        pip install --upgrade pip
        pip install -r ModelDevelopment/Vision/requirements.txt
        echo "Running bias detection..."
        cd ModelDevelopment/Vision
        
        # Find model in date-partitioned structure
        MODEL_PATH=$$(find /workspace/models -name "CNN_ResNet50_best.keras" -type f | head -1)
        METADATA_PATH=$$(find /workspace/models -name "training_metadata.json" -type f | head -1)
        
        if [ -z "$$MODEL_PATH" ] || [ ! -f "$$MODEL_PATH" ]; then
          echo "ERROR: Model file not found for bias check!"
          exit 1
        fi
        
        python bias_check.py \
          --model_path "$$MODEL_PATH" \
          --metadata_path "$$METADATA_PATH" \
          --output_file /workspace/bias_results.json
        
        echo "Bias check completed"

  # Step 7: Deploy to Vertex AI
  - name: 'python:3.10'
    id: 'deploy-to-vertex-ai'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        echo "Installing dependencies for deployment..."
        pip install --upgrade pip
        pip install -r ModelDevelopment/Vision/requirements.txt
        pip install google-cloud-aiplatform
        echo "ðŸš€ Deploying to Vertex AI..."
        cd ModelDevelopment/Vision
        
        # Find model in date-partitioned structure
        MODEL_PATH=$$(find /workspace/models -name "CNN_ResNet50_best.keras" -type f | head -1)
        METADATA_PATH=$$(find /workspace/models -name "training_metadata.json" -type f | head -1)
        
        if [ -z "$$MODEL_PATH" ] || [ ! -f "$$MODEL_PATH" ]; then
          echo "ERROR: Model file not found for deployment!"
          exit 1
        fi
        
        if [ -z "$$METADATA_PATH" ] || [ ! -f "$$METADATA_PATH" ]; then
          echo "ERROR: Metadata file not found for deployment!"
          exit 1
        fi
        
        TEST_ACCURACY=$$(python -c "import json; print(json.load(open('$$METADATA_PATH'))['metrics']['test_accuracy'])")
        
        # Set environment variables for deployment
        export GCP_PROJECT_ID=medscanai-476500
        export GCS_BUCKET_NAME=medscan-pipeline-medscanai-476500
        
        # Run deployment; fail fast if it errors
        python deploy.py \
          --model_path "$$MODEL_PATH" \
          --dataset ${_DATASET} \
          --model_name CNN_ResNet50 \
          --test_accuracy "$$TEST_ACCURACY" \
          --metadata_file "$$METADATA_PATH" || { echo "Deployment failed"; exit 1; }
        
        echo "Deployment completed successfully"

  # Step 8: Upload artifacts to GCS
  - name: 'gcr.io/cloud-builders/gsutil'
    id: 'upload-artifacts'
    args:
      - '-m'
      - 'cp'
      - '-r'
      - '/workspace/models/'
      - 'gs://medscan-pipeline-medscanai-476500/vision/trained_models/${BUILD_ID}/'

  # Step 9: Upload validation results
  - name: 'gcr.io/cloud-builders/gsutil'
    id: 'upload-logs'
    args:
      - 'cp'
      - '/workspace/validation_results.json'
      - 'gs://medscan-pipeline-medscanai-476500/vision/validation/${BUILD_ID}/'

  # Step 10: Send email notification on SUCCESS
  - name: 'python:3.10'
    id: 'send-success-email'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        echo "Sending success notification..."
        
        # Get test accuracy from metadata
        METADATA_PATH=$$(find /workspace/models -name "training_metadata.json" -type f | head -1)
        
        if [ -z "$$METADATA_PATH" ] || [ ! -f "$$METADATA_PATH" ]; then
          echo "ERROR: Metadata file not found for email notification!"
          exit 1
        fi
        TEST_ACCURACY=$$(python -c "import json; print(json.load(open('$$METADATA_PATH'))['metrics']['test_accuracy'])")
        
        # Install email dependencies
        pip install secure-smtplib
        
        # Create email script
        cat > /tmp/send_email.py << 'PYEOF'
        import smtplib
        import os
        from email.mime.text import MIMEText
        from email.mime.multipart import MIMEMultipart

        smtp_user = os.getenv('SMTP_USER')
        smtp_password = os.getenv('SMTP_PASSWORD')

        if not smtp_user or not smtp_password:
            print("No SMTP credentials - skipping email")
            exit(0)

        recipients = ['harshitha8.shekar@gmail.com', 'kothari.sau@northeastern.edu']

        msg = MIMEMultipart('alternative')
        msg['From'] = f'MedScan AI <{smtp_user}>'
        msg['To'] = ', '.join(recipients)
        msg['Subject'] = ' Vision Model Training Complete - ${_DATASET}'

        plain_text = """Vision Model Training Completed Successfully!

        Model Details:
        - Model: CNN_ResNet50
        - Dataset: ${_DATASET}
        - Test Accuracy: """ + os.getenv('TEST_ACCURACY', '0.0') + """%
        - Build ID: ${BUILD_ID}

        Results:
        - Validation: PASSED
        - Bias Check: COMPLETED
        - Deployment: SUCCESS

        View build:
        https://console.cloud.google.com/cloud-build/builds/${BUILD_ID}?project=medscanai-476500

        Model artifacts:
        gs://medscan-pipeline-medscanai-476500/vision/trained_models/${BUILD_ID}/

        ---
        MedScan AI - Automated Training System
        """

        html = """
        <html>
        <body style="font-family: Arial, sans-serif;">
            <div style="background-color: #4CAF50; color: white; padding: 20px;">
                <h2>Vision Model Training Complete</h2>
            </div>
            <div style="padding: 20px;">
                <h3>Model Details</h3>
                <ul>
                    <li><strong>Model:</strong> CNN_ResNet50</li>
                    <li><strong>Dataset:</strong> ${_DATASET}</li>
                    <li><strong>Test Accuracy:</strong> """ + os.getenv('TEST_ACCURACY', '0.0') + """%</li>
                    <li><strong>Build ID:</strong> ${BUILD_ID}</li>
                </ul>
                
                <h3>Results</h3>
                <ul style="color: green;">
                    <li>Validation: PASSED</li>
                    <li>Bias Check: COMPLETED</li>
                    <li>Deployment: SUCCESS</li>
                </ul>
                
                <p><a href="https://console.cloud.google.com/cloud-build/builds/${BUILD_ID}?project=medscanai-476500">View Build Logs â†’</a></p>
            </div>
            <div style="background-color: #f5f5f5; padding: 15px; text-align: center;">
                <p>MedScan AI - Automated Training System</p>
            </div>
        </body>
        </html>
        """

        msg.attach(MIMEText(plain_text, 'plain'))
        msg.attach(MIMEText(html, 'html'))

        try:
            with smtplib.SMTP('smtp.gmail.com', 587) as server:
                server.starttls()
                server.login(smtp_user, smtp_password)
                server.send_message(msg)
            print("Email sent successfully")
        except Exception as e:
            print(f"Email failed: {e}")
        PYEOF
        
        # Run email script with credentials from Secret Manager
        export SMTP_USER=$$(gcloud secrets versions access latest --secret="smtp-username" --project=medscanai-476500 2>/dev/null || echo "")
        export SMTP_PASSWORD=$$(gcloud secrets versions access latest --secret="smtp-password" --project=medscanai-476500 2>/dev/null || echo "")
        export TEST_ACCURACY="$$TEST_ACCURACY"
        
        python /tmp/send_email.py || echo "Email notification failed (non-critical)"

# Build configuration
timeout: 7200s

substitutions:
  _DATASET: 'tb'
  _EPOCHS: '50'
  # Note: MODEL_PATH is a bash variable set during execution, not a Cloud Build substitution
  # Note: MODEL_PATH, TEST_DATA, METADATA_PATH, TEST_ACCURACY are bash variables
  # Escaped with $$ to prevent Cloud Build from treating them as substitutions

options:
  machineType: 'E2_HIGHCPU_8'
  diskSizeGb: 100
  logging: CLOUD_LOGGING_ONLY
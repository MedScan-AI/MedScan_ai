# Cloud Build configuration for Vision model training
steps:
  # Step 1: Install dependencies
  - name: 'python:3.10'
    id: 'install-deps'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        echo "Installing dependencies..."
        pip install --upgrade pip
        pip install -r ModelDevelopment/Vision/requirements.txt
        echo "Dependencies installed"

  # Step 2: Install DVC and pull preprocessed data (combined - DVC must be in same step)
  - name: 'python:3.10'
    id: 'pull-dvc-data'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        echo "Installing DVC and git..."
        pip install --upgrade pip
        # Install DVC with GCS support
        pip install dvc dvc-gs
        # Install git for DVC compatibility (some versions require git even with --no-scm)
        apt-get update && apt-get install -y git || echo "Git installation failed (may already be installed)"
        echo "DVC, dvc-gs, and git installed"
        
        # Verify dvc-gs is installed
        python -c "import dvc_gs; print('dvc-gs installed successfully')" || echo "WARNING: dvc-gs import failed"
        
        echo "Configuring DVC remote..."
        cd /workspace/DataPipeline
        
        # Cloud Build runs with service account permissions
        # Check if credentials are available
        if [ -f /workspace/airflow/gcp-service-account.json ]; then
          export GOOGLE_APPLICATION_CREDENTIALS=/workspace/airflow/gcp-service-account.json
          echo "Using service account key from workspace"
        elif [ -n "$$GOOGLE_APPLICATION_CREDENTIALS" ]; then
          echo "Using GOOGLE_APPLICATION_CREDENTIALS from environment"
        else
          echo "Using Cloud Build default service account"
        fi
        
        # Initialize minimal git repo first (some DVC versions require it even with --no-scm)
        echo "Initializing minimal git repository for DVC compatibility..."
        if ! git rev-parse --git-dir >/dev/null 2>&1; then
          git init
          git config user.email "cloud-build@medscan.ai"
          git config user.name "Cloud Build"
          echo "Git repository initialized"
        else
          echo "Git repository already exists"
        fi
        
        # Initialize DVC with --no-scm (git exists but DVC won't use it for version control)
        echo "Initializing DVC in no-scm mode..."
        
        # Remove existing .dvc directory if it exists (from repo)
        # This ensures fresh initialization with --no-scm
        if [ -d .dvc ]; then
          echo "Removing existing .dvc directory for fresh initialization..."
          rm -rf .dvc
        fi
        
        # Initialize DVC with --no-scm flag (git exists but won't be used for DVC tracking)
        dvc init --no-scm
        
        # Verify DVC is properly initialized
        if [ ! -f .dvc/config ]; then
          echo "ERROR: DVC initialization failed"
          exit 1
        fi
        
        echo "DVC initialized successfully in no-scm mode"
        
        # Configure DVC remote
        dvc remote add -d vision gs://medscan-pipeline-medscanai-476500/dvc-storage/vision 2>/dev/null || \
        dvc remote modify vision url gs://medscan-pipeline-medscanai-476500/dvc-storage/vision
        
        # Set project for GCS access
        dvc remote modify vision projectname medscanai-476500 2>/dev/null || true
        
        # Verify remote configuration
        echo "DVC remote configuration:"
        dvc remote list || echo "Remote list failed (non-critical)"
        
        # Verify .dvc file exists (it should be in the uploaded source code)
        echo "Current directory: $$(pwd)"
        echo "Checking workspace structure:"
        ls -la /workspace/ | head -10
        echo ""
        echo "Checking DataPipeline structure:"
        ls -la /workspace/DataPipeline/ 2>/dev/null | head -10 || echo "DataPipeline not found at /workspace/DataPipeline"
        echo ""
        echo "Searching for .dvc files in workspace:"
        find /workspace -name "*.dvc" -type f 2>/dev/null | head -20
        echo ""
        echo "Checking if we're in the right location:"
        if [ -f data/preprocessed.dvc ]; then
          echo "Found data/preprocessed.dvc in current directory"
        else
          echo "data/preprocessed.dvc NOT found in current directory"
          echo "Trying to find it relative to workspace root..."
          # Check if we need to navigate from workspace root
          if [ -f /workspace/DataPipeline/data/preprocessed.dvc ]; then
            echo "Found at /workspace/DataPipeline/data/preprocessed.dvc"
            cd /workspace/DataPipeline
          else
            echo "ERROR: data/preprocessed.dvc file not found anywhere!"
            echo "Available .dvc files:"
            find /workspace -name "*.dvc" -type f 2>/dev/null | head -20
            exit 1
          fi
        fi
        
        echo "Pulling preprocessed data from DVC..."
        # Try pulling with explicit remote and no git check
        # Some DVC versions require git even with --no-scm, so we handle gracefully
        if ! dvc pull data/preprocessed.dvc -r vision --jobs 4 2>&1; then
          echo "DVC pull failed, checking error..."
          echo "If error is about git repository, DVC might need git even with --no-scm"
          echo "Attempting workaround: creating minimal git repo..."
          
          # Create minimal git repo if needed (DVC workaround)
          if ! git rev-parse --git-dir >/dev/null 2>&1; then
            echo "Initializing minimal git repository for DVC compatibility..."
            git init
            git config user.email "cloud-build@medscan.ai"
            git config user.name "Cloud Build"
            # Don't commit anything, just initialize
          fi
          
          # Retry pull after git init
          echo "Retrying DVC pull..."
          dvc pull data/preprocessed.dvc -r vision --jobs 4
          
          if [ $$? -ne 0 ]; then
            echo "ERROR: DVC pull failed even after git initialization"
            exit 1
          fi
        fi
        
        echo "DVC pull completed successfully"
        
        echo "Data pulled successfully"
        echo "Checking downloaded data structure..."
        find data/preprocessed -type d -maxdepth 2 2>/dev/null | head -10
        find data/preprocessed -type f -name "*.jpg" 2>/dev/null | head -5

  # Step 3: Verify DVC data downloaded
  - name: 'ubuntu'
    id: 'verify-data'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        echo "Verifying DVC data..."
        cd /workspace/DataPipeline
        
        # Check if data exists
        if [ ! -d "data/preprocessed" ]; then
          echo "ERROR: data/preprocessed directory not found!"
          exit 1
        fi
        
        echo "Preprocessed data structure:"
        ls -la data/preprocessed/ | head -10
        
        echo "Finding dataset directories..."
        find data/preprocessed -type d -maxdepth 2 | head -10
        
        echo "Counting images..."
        TB_COUNT=$$(find data/preprocessed/tb -name "*.jpg" 2>/dev/null | wc -l)
        LC_COUNT=$$(find data/preprocessed/lung_cancer* -name "*.jpg" 2>/dev/null | wc -l)
        echo "TB images: $$TB_COUNT"
        echo "Lung Cancer images: $$LC_COUNT"
        
        if [ "$$TB_COUNT" -eq 0 ] && [ "$$LC_COUNT" -eq 0 ]; then
          echo "ERROR: No images found in preprocessed data!"
          exit 1
        fi
        
        # Create symlink/copy for training script compatibility
        mkdir -p /workspace/data
        if [ ! -e "/workspace/data/preprocessed" ]; then
          ln -sf /workspace/DataPipeline/data/preprocessed /workspace/data/preprocessed || \
          cp -r /workspace/DataPipeline/data/preprocessed /workspace/data/preprocessed
        fi

  # Step 4: Train all 3 models (ResNet18, ViT, Custom CNN) in parallel
  # Each model includes validation and bias detection
  - name: 'python:3.10'
    id: 'train-resnet'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        echo "Installing dependencies for training..."
        pip install --upgrade pip
        pip install -r ModelDevelopment/Vision/requirements.txt
        echo "Starting ResNet18 training..."
        echo "Note: Validation and bias detection are integrated into training"
        cd ModelDevelopment/Vision
        python train_resnet.py \
          --config ../config/vision_training.yml \
          --data_path /workspace/DataPipeline/data/preprocessed \
          --output_path /workspace/models \
          --datasets ${_DATASET} \
          --epochs ${_EPOCHS}
        echo "ResNet18 training completed"
    env:
      - 'TF_CPP_MIN_LOG_LEVEL=2'
      - 'PYTHONUNBUFFERED=1'

  - name: 'python:3.10'
    id: 'train-vit'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        echo "Installing dependencies for training..."
        pip install --upgrade pip
        pip install -r ModelDevelopment/Vision/requirements.txt
        echo "Starting ViT training..."
        echo "Note: Validation and bias detection are integrated into training"
        cd ModelDevelopment/Vision
        python train_vit.py \
          --config ../config/vision_training.yml \
          --data_path /workspace/DataPipeline/data/preprocessed \
          --output_path /workspace/models \
          --datasets ${_DATASET} \
          --epochs ${_EPOCHS}
        echo "ViT training completed"
    env:
      - 'TF_CPP_MIN_LOG_LEVEL=2'
      - 'PYTHONUNBUFFERED=1'

  - name: 'python:3.10'
    id: 'train-custom-cnn'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        echo "Installing dependencies for training..."
        pip install --upgrade pip
        pip install -r ModelDevelopment/Vision/requirements.txt
        echo "Starting Custom CNN training..."
        echo "Note: Validation and bias detection are integrated into training"
        cd ModelDevelopment/Vision
        python train_custom_cnn.py \
          --config ../config/vision_training.yml \
          --data_path /workspace/DataPipeline/data/preprocessed \
          --output_path /workspace/models \
          --datasets ${_DATASET} \
          --epochs ${_EPOCHS}
        echo "Custom CNN training completed"
    env:
      - 'TF_CPP_MIN_LOG_LEVEL=2'
      - 'PYTHONUNBUFFERED=1'

  # Step 5: Select best model from the 3 trained models
  - name: 'python:3.10'
    id: 'select-best-model'
    waitFor: ['train-resnet', 'train-vit', 'train-custom-cnn']
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        echo "Selecting best model from trained models..."
        cd ModelDevelopment/Vision
        pip install --upgrade pip
        # Only need standard library for this script
        python select_best_model.py \
          --output_path /workspace/models \
          --dataset ${_DATASET} \
          --metric test_accuracy
        echo "Model selection completed"
        
        # Verify selection result exists
        if [ ! -f /workspace/models/model_selection_result.json ]; then
          echo "ERROR: Model selection result not found!"
          exit 1
        fi
        
        echo "Best model selected and saved to model_selection_result.json"

  # Step 6: Extract validation and bias results from best model
  - name: 'python:3.10'
    id: 'extract-results'
    waitFor: ['select-best-model']
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        echo "Extracting validation and bias results from best model..."
        cd ModelDevelopment/Vision
        
        # Read best model info from selection result
        SELECTION_RESULT="/workspace/models/model_selection_result.json"
        if [ ! -f "$$SELECTION_RESULT" ]; then
          echo "ERROR: Model selection result not found!"
          exit 1
        fi
        
        # Extract best model metadata path
        METADATA_PATH=$$(python -c "
        import json
        with open('$$SELECTION_RESULT', 'r') as f:
            result = json.load(f)
        print(result['best_model']['metadata_path'])
        ")
        
        MODEL_NAME=$$(python -c "
        import json
        with open('$$SELECTION_RESULT', 'r') as f:
            result = json.load(f)
        print(result['best_model']['model_name'])
        ")
        
        if [ -z "$$METADATA_PATH" ] || [ ! -f "$$METADATA_PATH" ]; then
          echo "ERROR: Best model metadata not found at: $$METADATA_PATH"
          exit 1
        fi
        
        # Extract validation metrics from best model metadata
        python -c "
        import json
        with open('$$METADATA_PATH', 'r') as f:
            metadata = json.load(f)
        metrics = metadata.get('metrics', {})
        test_accuracy = metrics.get('test_accuracy', 0.0)
        validation_result = {
            'test_accuracy': test_accuracy,
            'test_precision': metrics.get('test_precision', 0.0),
            'test_recall': metrics.get('test_recall', 0.0),
            'test_f1_score': metrics.get('test_f1_score', 0.0),
            'validation_passed': test_accuracy >= 0.5
        }
        with open('/workspace/validation_results.json', 'w') as f:
            json.dump(validation_result, f, indent=2)
        print('Validation results extracted from best model')
        "
        
        # Save test accuracy and model name for validation step
        TEST_ACCURACY=$$(python -c "import json; print(json.load(open('/workspace/validation_results.json'))['test_accuracy'])")
        echo "$$TEST_ACCURACY" > /workspace/test_accuracy.txt
        echo "$$MODEL_NAME" > /workspace/model_name.txt
        
        # Check if bias reports exist (look in best model's directory)
        METADATA_DIR=$$(dirname "$$METADATA_PATH")
        BIAS_REPORT_DIR=$$(find "$$METADATA_DIR" -type d -name "bias_reports" | head -1)
        if [ -n "$$BIAS_REPORT_DIR" ]; then
          echo "Bias reports found at: $$BIAS_REPORT_DIR"
          BIAS_REPORT=$$(find "$$BIAS_REPORT_DIR" -name "bias_report_*.json" | head -1)
          if [ -n "$$BIAS_REPORT" ]; then
            cp "$$BIAS_REPORT" /workspace/bias_results.json
            echo "Bias results extracted"
          else
            echo "No bias report JSON found, creating empty result"
            echo '{"bias_detected": false, "num_violations": 0, "violations": []}' > /workspace/bias_results.json
          fi
        else
          echo "No bias reports directory found, creating empty result"
          echo '{"bias_detected": false, "num_violations": 0, "violations": []}' > /workspace/bias_results.json
        fi
        
        echo "Results extraction completed"

  # Step 7: Validate model performance (blocks deployment if validation fails)
  - name: 'python:3.10'
    id: 'validate-model'
    waitFor: ['extract-results']
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        echo "Validating model performance"
        cd ModelDevelopment/Vision
        
        # Read validation results
        TEST_ACCURACY=$$(cat /workspace/test_accuracy.txt)
        MODEL_NAME=$$(cat /workspace/model_name.txt)
        # Use lower threshold for quick tests (2 epochs), higher for production (50 epochs)
        # Check epochs from substitution - if 2 or less, use 0.2; otherwise 0.5
        EPOCHS=${_EPOCHS:-50}
        echo "EPOCHS value: $$EPOCHS"
        # Convert to integer for comparison (handle string "2" vs integer 2)
        EPOCHS_INT=$$(echo "$$EPOCHS" | awk '{print int($$1)}')
        if [ $$EPOCHS_INT -le 2 ]; then
          VALIDATION_THRESHOLD=0.2  # Lower threshold for quick tests
          echo "Using lower validation threshold (0.2) for quick test with $$EPOCHS epochs"
        else
          VALIDATION_THRESHOLD=0.5  # Standard threshold for production
          echo "Using standard validation threshold (0.5) for production training with $$EPOCHS epochs"
        fi
        
        echo "Test Accuracy: $$TEST_ACCURACY"
        echo "Validation Threshold: $$VALIDATION_THRESHOLD"
        
        # Check if accuracy meets threshold
        VALIDATION_PASSED=$$(python -c "score=$$TEST_ACCURACY; threshold=$$VALIDATION_THRESHOLD; print('true' if score >= threshold else 'false')")
        
        echo "$$VALIDATION_PASSED" > /workspace/validation_passed.txt
        
        if [ "$$VALIDATION_PASSED" = "false" ]; then
          echo "VALIDATION FAILED: Test accuracy ($$TEST_ACCURACY) is below threshold ($$VALIDATION_THRESHOLD)"
          
          # Send validation failure email
          pip install --upgrade pip
          pip install secure-smtplib>=0.1.1
          
          SMTP_USER=$$(gcloud secrets versions access latest --secret="smtp-username" --project=medscanai-476500 2>&1)
          SMTP_PASSWORD=$$(gcloud secrets versions access latest --secret="smtp-password" --project=medscanai-476500 2>&1)
          
          if echo "$$SMTP_USER" | grep -q "ERROR\|Permission denied\|not found"; then
            SMTP_USER=""
            SMTP_PASSWORD=""
          fi
          
          export SMTP_USER="$$SMTP_USER"
          export SMTP_PASSWORD="$$SMTP_PASSWORD"
          export GCP_PROJECT_ID=medscanai-476500
          
          python utils/send_notification.py \
            --type validation-failure \
            --build-id ${BUILD_ID} \
            --model-name "$$MODEL_NAME" \
            --dataset ${_DATASET} \
            --test-accuracy $$TEST_ACCURACY \
            --threshold $$VALIDATION_THRESHOLD || echo "Validation failure email failed (non-critical)"
          
          exit 1
        fi
        
        echo "Validation PASSED: Model meets minimum performance threshold"

  # Step 8: Deploy best model to Vertex AI (only if validation passed)
  - name: 'python:3.10'
    id: 'deploy-to-vertex-ai'
    waitFor: ['validate-model']
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        echo "Installing dependencies for deployment..."
        pip install --upgrade pip
        pip install -r ModelDevelopment/Vision/requirements.txt
        pip install google-cloud-aiplatform
        echo "ðŸš€ Deploying best model to Vertex AI..."
        cd ModelDevelopment/Vision
        
        # Read best model info from selection result
        SELECTION_RESULT="/workspace/models/model_selection_result.json"
        if [ ! -f "$$SELECTION_RESULT" ]; then
          echo "ERROR: Model selection result not found!"
          exit 1
        fi
        
        # Extract best model path and metadata
        MODEL_PATH=$$(python -c "
        import json
        with open('$$SELECTION_RESULT', 'r') as f:
            result = json.load(f)
        print(result['best_model']['model_path'])
        ")
        
        METADATA_PATH=$$(python -c "
        import json
        with open('$$SELECTION_RESULT', 'r') as f:
            result = json.load(f)
        print(result['best_model']['metadata_path'])
        ")
        
        MODEL_NAME=$$(python -c "
        import json
        with open('$$SELECTION_RESULT', 'r') as f:
            result = json.load(f)
        print(result['best_model']['model_name'])
        ")
        
        if [ -z "$$MODEL_PATH" ] || [ ! -f "$$MODEL_PATH" ]; then
          echo "ERROR: Best model file not found at: $$MODEL_PATH"
          find /workspace/models -name "*.keras" -type f 2>/dev/null | head -5
          exit 1
        fi
        
        if [ -z "$$METADATA_PATH" ] || [ ! -f "$$METADATA_PATH" ]; then
          echo "ERROR: Best model metadata not found at: $$METADATA_PATH"
          exit 1
        fi
        
        echo "Deploying model: $$MODEL_NAME"
        echo "Model path: $$MODEL_PATH"
        echo "Metadata path: $$METADATA_PATH"
        
        # Set environment variables for deployment
        export GCP_PROJECT_ID=medscanai-476500
        export GCS_BUCKET_NAME=medscan-pipeline-medscanai-476500
        export BUILD_ID=${BUILD_ID}
        
        # Run deployment; fail fast if it errors
        python deploy.py \
          --model_path "$$MODEL_PATH" \
          --metadata_file "$$METADATA_PATH" \
          --dataset ${_DATASET} \
          --model_name "$$MODEL_NAME" || { echo "Deployment failed"; exit 1; }
        
        echo "Deployment completed successfully"

  # Step 9: Upload artifacts to GCS (includes models, metadata, bias reports, selection results)
  - name: 'gcr.io/cloud-builders/gsutil'
    id: 'upload-artifacts'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        echo "Uploading artifacts to GCS..."
        BUCKET="gs://medscan-pipeline-medscanai-476500"
        BUILD_DIR="vision/trained_models/${BUILD_ID}"
        
        # Upload models directory
        gsutil -m cp -r /workspace/models/ $$BUCKET/$$BUILD_DIR/
        
        # Upload validation results
        if [ -f /workspace/validation_results.json ]; then
          gsutil cp /workspace/validation_results.json $$BUCKET/vision/validation/${BUILD_ID}/validation_results.json
        fi
        
        # Upload bias results
        if [ -f /workspace/bias_results.json ]; then
          gsutil cp /workspace/bias_results.json $$BUCKET/vision/validation/${BUILD_ID}/bias_results.json
        fi
        
        # Upload model selection results
        if [ -f /workspace/models/model_selection_result.json ]; then
          gsutil cp /workspace/models/model_selection_result.json $$BUCKET/vision/validation/${BUILD_ID}/model_selection_result.json
        fi
        
        echo "Artifacts uploaded successfully"

  # Step 10: Send email notifications (completion + bias failure if needed)
  - name: 'python:3.10'
    id: 'send-completion-email'
    waitFor: ['deploy-to-vertex-ai']
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        echo "Sending training completion notification"
        
        pip install --upgrade pip
        cd ModelDevelopment/Vision
        pip install secure-smtplib>=0.1.1
        
        # Get best model info from selection result
        SELECTION_RESULT="/workspace/models/model_selection_result.json"
        if [ -f "$$SELECTION_RESULT" ]; then
          MODEL_NAME=$$(python -c "import json; print(json.load(open('$$SELECTION_RESULT'))['best_model']['model_name'])")
          TEST_ACCURACY=$$(python -c "import json; print(json.load(open('$$SELECTION_RESULT'))['best_model']['test_accuracy'])")
        else
          MODEL_NAME="Unknown"
          TEST_ACCURACY="0.0"
        fi
        
        # Get validation status
        VALIDATION_PASSED=$$(cat /workspace/validation_passed.txt 2>/dev/null || echo "true")
        
        # Get bias status
        if [ -f /workspace/bias_results.json ]; then
          BIAS_DETECTED=$$(python -c "import json; print(json.load(open('/workspace/bias_results.json')).get('bias_detected', False))")
        else
          BIAS_DETECTED="false"
        fi
        
        # Get SMTP credentials from Secret Manager
        SMTP_USER=$$(gcloud secrets versions access latest --secret="smtp-username" --project=medscanai-476500 2>&1)
        SMTP_PASSWORD=$$(gcloud secrets versions access latest --secret="smtp-password" --project=medscanai-476500 2>&1)
        
        if echo "$$SMTP_USER" | grep -q "ERROR\|Permission denied\|not found"; then
          echo "Warning: Could not retrieve SMTP credentials from Secret Manager"
          SMTP_USER=""
          SMTP_PASSWORD=""
        fi
        
        export SMTP_USER="$$SMTP_USER"
        export SMTP_PASSWORD="$$SMTP_PASSWORD"
        export GCP_PROJECT_ID=medscanai-476500
        export GCS_BUCKET_NAME=medscan-pipeline-medscanai-476500
        
        # Determine bias check passed (opposite of bias detected)
        if [ "$$BIAS_DETECTED" = "True" ]; then
          BIAS_CHECK_PASSED="false"
          # Send bias failure email
          python utils/send_notification.py \
            --type bias-failure \
            --build-id ${BUILD_ID} \
            --model-name "$$MODEL_NAME" \
            --dataset ${_DATASET} \
            --bias-details /workspace/bias_results.json || echo "Bias failure email failed (non-critical)"
        else
          BIAS_CHECK_PASSED="true"
        fi
        
        # Send completion email
        python utils/send_notification.py \
          --type completion \
          --build-id ${BUILD_ID} \
          --model-name "$$MODEL_NAME" \
          --dataset ${_DATASET} \
          --test-accuracy $$TEST_ACCURACY \
          --validation-passed "$$VALIDATION_PASSED" \
          --bias-check-passed "$$BIAS_CHECK_PASSED" \
          --bias-details /workspace/bias_results.json || echo "Completion email failed (non-critical)"

# Note: Pipeline failure emails are handled by individual steps that write to /workspace/pipeline_error.txt
# Critical steps (train-resnet, train-vit, train-custom-cnn, select-best-model, validate-model, deploy-to-vertex-ai)
# should write error messages before exiting with error code

# Build configuration
timeout: 7200s

substitutions:
  _DATASET: 'tb'
  _EPOCHS: '50'
  # Note: MODEL_PATH is a bash variable set during execution, not a Cloud Build substitution
  # Note: MODEL_PATH, TEST_DATA, METADATA_PATH, TEST_ACCURACY are bash variables
  # Escaped with $$ to prevent Cloud Build from treating them as substitutions

options:
  machineType: 'E2_HIGHCPU_8'
  diskSizeGb: 100
  logging: CLOUD_LOGGING_ONLY
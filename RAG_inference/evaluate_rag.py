"""
evaluate_rag.py - MLflow-integrated RAG evaluation with hyperparameter tuning
Follows PEP 8 style guide
"""

import json

from transformers.utils import logging
logging.set_verbosity_error() 

import logging
import sys
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional
from dataclasses import dataclass
from datetime import datetime
import itertools
import re
import mlflow
import mlflow.sklearn
import numpy as np
import torch

from transformers import AutoModelForSequenceClassification, AutoTokenizer
from config import ExperimentConfig, HYPERPARAMETER_GRID
from models import ModelFactory
from retrieval import DocumentRetriever, DocumentRecord, get_embeddings
from prompts import PROMPT_TEMPLATES
# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('rag_evaluation.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

def _cosine_sim(a: np.ndarray, b: np.ndarray) -> float:
    """
    Compute cosine similarity between two vectors
    
    Args:
        a: First vector
        b: Second vector
        
    Returns:
        Cosine similarity (0 to 1)
    """
    denom = np.linalg.norm(a) * np.linalg.norm(b)
    if denom == 0:
        return 0.0
    return float(np.dot(a, b) / denom)


def compute_semantic_similarity(
    generated_answer: str,
    reference_answer: str
) -> float:
    """
    Compute cosine similarity between embeddings of generated and reference answers
    
    This measures how semantically similar the generated answer is to the 
    reference answer using dense embeddings.
    
    Args:
        generated_answer: The answer generated by the model
        reference_answer: The ground truth reference answer
        
    Returns:
        float: Cosine similarity score between 0 and 1
               - 1.0 means identical semantic meaning
               - 0.0 means completely different
               - Higher is better

    """
    try:
        # Get embeddings for both texts
        gen_emb = get_embeddings(generated_answer)
        ref_emb = get_embeddings(reference_answer)
        
        # Compute cosine similarity
        similarity = _cosine_sim(gen_emb, ref_emb)
        
        return similarity
        
    except Exception as e:
        logger.error(f"Error computing semantic similarity: {str(e)}")
        return 0.0



def compute_answer_relevance(
    generated_answer: str,
    reference_answer: str
) -> Dict[str, float]:
    """
    Compute keyword-based relevance metrics between generated and reference answers
    
    This uses lexical matching to measure overlap at word and phrase level.
    Complements semantic similarity with explicit keyword matching.
    
    Args:
        generated_answer: The answer generated by the model
        reference_answer: The ground truth reference answer
        
    Returns:
        Dict[str, float]: Dictionary containing:
            - keyword_precision: How many keywords in generated are in reference (0-1)
            - keyword_recall: How many keywords from reference are in generated (0-1)
            - keyword_f1: Harmonic mean of precision and recall (0-1)
            - unigram_overlap: Jaccard similarity of word sets (0-1)
            - bigram_overlap: Jaccard similarity of consecutive word pairs (0-1)
            
            Higher values are better for all metrics.
    """
    
    def preprocess(text: str) -> List[str]:
        """
        Preprocess text: lowercase, remove punctuation, filter stop words
        
        Args:
            text: Input text
            
        Returns:
            List of cleaned words
        """
        # Lowercase
        text = text.lower()
        
        # Remove punctuation (keep only alphanumeric and spaces)
        text = re.sub(r'[^\w\s]', ' ', text)
        
        # Split into words
        words = text.split()
        
        # Define stop words (common words to ignore)
        stop_words = {
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at',
            'to', 'for', 'of', 'with', 'by', 'from', 'is', 'was',
            'are', 'were', 'been', 'be', 'have', 'has', 'had', 'do',
            'does', 'did', 'will', 'would', 'could', 'should', 'may',
            'might', 'can', 'this', 'that', 'these', 'those', 'it'
        }
        
        # Filter: remove stop words and words shorter than 3 characters
        filtered = [w for w in words if w not in stop_words and len(w) > 2]
        
        return filtered
    
    def get_bigrams(words: List[str]) -> List[Tuple[str, str]]:
        """
        Extract bigrams (consecutive word pairs) from word list
        
        Args:
            words: List of words
            
        Returns:
            List of bigram tuples
        """
        return [(words[i], words[i+1]) for i in range(len(words)-1)]
    
    # Preprocess both answers
    gen_words = preprocess(generated_answer)
    ref_words = preprocess(reference_answer)
    
    # Handle empty texts
    if not gen_words or not ref_words:
        return {
            "keyword_precision": 0.0,
            "keyword_recall": 0.0,
            "keyword_f1": 0.0,
            "unigram_overlap": 0.0,
            "bigram_overlap": 0.0
        }
    
    # Convert to sets for comparison
    gen_set = set(gen_words)
    ref_set = set(ref_words)
    
    # Compute unigram metrics
    common_words = gen_set.intersection(ref_set)
    
    # Precision: How many of generated words are in reference
    precision = len(common_words) / len(gen_set) if gen_set else 0.0
    
    # Recall: How many of reference words are in generated
    recall = len(common_words) / len(ref_set) if ref_set else 0.0
    
    # F1: Harmonic mean of precision and recall
    if (precision + recall) > 0:
        f1 = 2 * (precision * recall) / (precision + recall)
    else:
        f1 = 0.0
    
    # Jaccard similarity for unigrams
    union = gen_set.union(ref_set)
    unigram_jaccard = len(common_words) / len(union) if union else 0.0
    
    # Compute bigram overlap
    gen_bigrams = set(get_bigrams(gen_words))
    ref_bigrams = set(get_bigrams(ref_words))
    
    if gen_bigrams and ref_bigrams:
        common_bigrams = gen_bigrams.intersection(ref_bigrams)
        bigram_union = gen_bigrams.union(ref_bigrams)
        bigram_jaccard = (
            len(common_bigrams) / len(bigram_union) 
            if bigram_union else 0.0
        )
    else:
        bigram_jaccard = 0.0
    
    return {
        "keyword_precision": precision,
        "keyword_recall": recall,
        "keyword_f1": f1,
        "unigram_overlap": unigram_jaccard,
        "bigram_overlap": bigram_jaccard
    }


def get_hallucination_score(
    generated_text: str,
    context_docs: list,
    aggregation: str = "mean"
) -> float:
    """
    Free hallucination scoring using NLI (DeBERTa MNLI).
    Checks if generated_text is supported by each context document.
    """
    
    model_name = "microsoft/deberta-base-mnli"
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForSequenceClassification.from_pretrained(model_name)

    except Exception as e:
        logger.error(f"Failed to load NLI model: {str(e)}")
        return 0.5
    
    if torch.cuda.is_available():
        model = model.to("cuda")
    
    if not context_docs:
        return 0.0
    
    scores = []
    
    for doc in context_docs:
        pair = (doc, generated_text)
        
        inputs = tokenizer(
            pair[0],
            pair[1],
            return_tensors="pt",
            truncation=True,
            max_length=512
        )
        
        if torch.cuda.is_available():
            inputs = {k: v.to("cuda") for k, v in inputs.items()}
        
        with torch.no_grad():
            logits = model(**inputs).logits
        
        probs = torch.softmax(logits, dim=1).cpu().numpy()[0]
        
        # MNLI label mapping:
        # 0 = contradiction
        # 1 = neutral
        # 2 = entailment
        entail = float(probs[2])
        
        scores.append(entail)
    
    if aggregation == "min":
        return min(scores)
    if aggregation == "max":
        return max(scores)
    
    return sum(scores) / len(scores)

def compute_retrieval_score(
    retrieved_docs: List[str],
    relevant_docs: List[str]
) -> float:
    """
    Compute precision of document retrieval
    
    Measures how many of the retrieved documents are actually relevant.
    This is a retrieval quality metric independent of answer generation.
    
    Args:
        retrieved_docs: List of documents retrieved by the system
        relevant_docs: List of ground truth relevant documents
        
    Returns:
        float: Retrieval precision between 0 and 1
               - 1.0 means all retrieved documents are relevant
               - 0.0 means no retrieved documents are relevant
               - Higher is better
               - Formula: |retrieved âˆ© relevant| / |retrieved|

    """
    
    # Handle empty inputs
    if not retrieved_docs or not relevant_docs:
        return 0.0
    
    # Convert to sets for set operations
    retrieved_set = set(retrieved_docs)
    relevant_set = set(relevant_docs)
    
    # Find intersection: documents that are both retrieved AND relevant
    correct = len(retrieved_set.intersection(relevant_set))
    
    # Precision: correct / total retrieved
    precision = correct / len(retrieved_docs)
    
    return precision


def compute_all_metrics(
    generated_answer: str,
    reference_answer: str,
    retrieved_docs: List[str],
    relevant_docs: Optional[List[str]] = None
) -> Dict[str, float]:
    """
    Compute all evaluation metrics at once
    
    Args:
        generated_answer: Generated answer text
        reference_answer: Ground truth answer
        retrieved_docs: Retrieved document texts
        relevant_docs: Ground truth relevant documents (optional)
        
    Returns:
        Dictionary with all metric scores
    """
    metrics = {}
    
    # Semantic similarity
    metrics['semantic_similarity'] = compute_semantic_similarity(
        generated_answer,
        reference_answer
    )
    
    # Keyword-based relevance
    relevance = compute_answer_relevance(
        generated_answer,
        reference_answer
    )
    metrics.update(relevance)
    
    # Hallucination score
    metrics['hallucination_score'] = get_hallucination_score(
        generated_answer,
        retrieved_docs
    )
    
    # Retrieval score (if relevant docs provided)
    if relevant_docs:
        metrics['retrieval_score'] = compute_retrieval_score(
            retrieved_docs,
            relevant_docs
        )
    
    return metrics

@dataclass
class QAPair:
    """Question-Answer pair for evaluation"""
    question: str
    reference_answer: str
    question_id: str = ""
    
    @classmethod
    def from_line(cls, line: str, idx: int) -> Optional['QAPair']:
        """
        Parse QA pair from line format:
        Q{N}: {question}? A: {answer}
        """
        try:
            # Split on 'A:' to separate question and answer
            parts = line.split(' A: ')
            if len(parts) != 2:
                logger.warning(f"Invalid format in line {idx}: {line[:50]}")
                return None
            
            # Extract question (remove Q{N}: prefix)
            q_part = parts[0].strip()
            if ':' in q_part:
                question = q_part.split(':', 1)[1].strip()
            else:
                question = q_part.strip()
            
            # Extract answer
            answer = parts[1].strip()
            
            return cls(
                question=question,
                reference_answer=answer,
                question_id=f"Q{idx}"
            )
        except Exception as e:
            logger.error(f"Error parsing line {idx}: {str(e)}")
            return None


class RAGEvaluator:
    """RAG system evaluator with MLflow tracking"""
    
    def __init__(self, config: ExperimentConfig):
        """
        Initialize evaluator
        
        Args:
            config: Experiment configuration
        """
        self.config = config
        self.retriever: Optional[DocumentRetriever] = None
        self.model: Optional[Any] = None
        self.qa_pairs: List[QAPair] = []
        
        # Setup MLflow
        mlflow.set_tracking_uri(str(config.paths.mlflow_tracking_uri))
        mlflow.set_experiment(config.mlflow.experiment_name)
    
    def load_components(self) -> None:
        """Load retriever, model, and QA pairs"""
        logger.info("Loading components...")
        
        # Load retriever
        self.retriever = DocumentRetriever(
            index_path=self.config.paths.index_file,
            chunks_path=self.config.paths.chunks_file,
            original_data_path=self.config.paths.original_data_file
        )
        logger.info(f"Retriever loaded: {self.retriever.get_stats()}")
        
        # Load model using factory
        self.model = ModelFactory.create_model(
            model_key=self.config.model.name,  # Can be key or full name
            max_tokens=self.config.model.max_tokens,
            temperature=self.config.model.temperature,
            top_p=self.config.model.top_p
        )
        logger.info(f"Model loaded: {self.config.model.name}")
        
        # Load QA pairs
        self.qa_pairs = self._load_qa_pairs(self.config.paths.qa_file)
        logger.info(f"Loaded {len(self.qa_pairs)} QA pairs")
    
    def _load_qa_pairs(self, qa_file: Path) -> List[QAPair]:
        """
        Load QA pairs from file
        
        Args:
            qa_file: Path to QA file
            
        Returns:
            List of QAPair objects
        """
        qa_pairs = []
        
        try:
            with open(qa_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            for idx, line in enumerate(lines, 1):
                line = line.strip()
                if not line or line.startswith('#'):
                    continue
                
                qa_pair = QAPair.from_line(line, idx)
                if qa_pair:
                    qa_pairs.append(qa_pair)
            
            logger.info(f"Successfully loaded {len(qa_pairs)} QA pairs")
            return qa_pairs
            
        except Exception as e:
            logger.error(f"Failed to load QA pairs: {str(e)}")
            return []
    
    def retrieve_documents(
        self,
        query: str,
        num_docs: int
    ) -> List[DocumentRecord]:
        """
        Retrieve documents for query
        
        Args:
            query: User query
            num_docs: Number of documents to retrieve
            
        Returns:
            List of retrieved documents
        """
        try:
            # Get query embedding
            query_embedding = get_embeddings(query)
            
            # Retrieve
            documents = self.retriever.retrieve(
                query_embedding=query_embedding,
                num_docs=num_docs,
                threshold=self.config.retrieval.similarity_threshold
            )
            
            return documents
            
        except Exception as e:
            logger.error(f"Retrieval error: {str(e)}")
            return []
    
    def generate_answer(
        self,
        query: str,
        documents: List[DocumentRecord],
        prompt_template: str
    ) -> Dict[str, Any]:
        """
        Generate answer using model
        
        Args:
            query: User query
            documents: Retrieved documents
            prompt_template: Prompt template to use
            
        Returns:
            Generation result dictionary
        """
        try:
            # Format context
            context = self.retriever.format_context_for_prompt(documents)
            
            # Build prompt
            prompt = prompt_template.replace("{context}", context).replace("{query}", query)
            
            # Generate
            result = self.model.infer(
                query=query,
                prompt=prompt,
                temperature=self.config.model.temperature,
                top_p=self.config.model.top_p,
                max_tokens=self.config.model.max_tokens
            )
            
            return result
            
        except Exception as e:
            logger.error(f"Generation error: {str(e)}")
            return {
                "generated_text": f"Error: {str(e)}",
                "input_tokens": 0,
                "output_tokens": 0,
                "success": False
            }
    
    def compute_metrics(
        self,
        generated_answer: str,
        reference_answer: str,
        retrieved_docs: List[DocumentRecord],
        generation_result: Dict[str, Any]
    ) -> Dict[str, float]:
        """
        Compute all evaluation metrics
        
        Args:
            generated_answer: Generated answer
            reference_answer: Reference answer
            retrieved_docs: Retrieved documents
            generation_result: Generation result dict
            
        Returns:
            Dictionary of metrics
        """
        metrics = {}
        
        try:
            # Semantic similarity
            metrics["semantic_similarity"] = compute_semantic_similarity(
                generated_answer,
                reference_answer
            )
            
            # Answer relevance metrics
            relevance_metrics = compute_answer_relevance(
                generated_answer,
                reference_answer
            )
            metrics.update(relevance_metrics)
            
            # Hallucination score
            doc_texts = [doc.content for doc in retrieved_docs]
            metrics["hallucination_score"] = get_hallucination_score(
                generated_answer,
                doc_texts
            )
            
            # Token counts
            metrics["input_tokens"] = generation_result.get("input_tokens", 0)
            metrics["output_tokens"] = generation_result.get("output_tokens", 0)
            
            # Success rate
            metrics["success"] = 1.0 if generation_result.get("success", False) else 0.0
            
        except Exception as e:
            logger.error(f"Metric computation error: {str(e)}")
            # Return default metrics
            metrics = {
                "semantic_similarity": 0.0,
                "keyword_precision": 0.0,
                "keyword_recall": 0.0,
                "keyword_f1": 0.0,
                "unigram_overlap": 0.0,
                "bigram_overlap": 0.0,
                "hallucination_score": 0.0,
                "input_tokens": 0,
                "output_tokens": 0,
                "success": 0.0
            }
        
        return metrics
    
    def evaluate_single_query(
        self,
        qa_pair: QAPair,
        num_docs: int,
        prompt_template: str
    ) -> Dict[str, Any]:
        """
        Evaluate single query
        
        Args:
            qa_pair: Question-answer pair
            num_docs: Number of documents to retrieve
            prompt_template: Prompt template
            
        Returns:
            Evaluation results
        """
        logger.info(f"Evaluating: {qa_pair.question_id}")
        
        # Retrieve documents
        documents = self.retrieve_documents(qa_pair.question, num_docs)
        
        # Generate answer
        generation_result = self.generate_answer(
            qa_pair.question,
            documents,
            prompt_template
        )
        
        # Compute metrics
        metrics = self.compute_metrics(
            generation_result["generated_text"],
            qa_pair.reference_answer,
            documents,
            generation_result
        )
        
        return {
            "question_id": qa_pair.question_id,
            "question": qa_pair.question,
            "reference_answer": qa_pair.reference_answer,
            "generated_answer": generation_result["generated_text"],
            "retrieved_docs": [
                {
                    "title": doc.title,
                    "metadata": doc.get_metadata_str(),
                    "content_preview": doc.content[:200]
                }
                for doc in documents
            ],
            "metrics": metrics
        }
    
    def run_evaluation(self) -> Dict[str, Any]:
        """
        Run evaluation on all QA pairs
        
        Returns:
            Aggregated results
        """
        logger.info("Starting evaluation...")
        
        # Get prompt template
        prompt_template = PROMPT_TEMPLATES.get(
            self.config.prompt.prompt_id,
            PROMPT_TEMPLATES["prompt1"]
        )
        
        results = []
        all_metrics = []
        
        for qa_pair in self.qa_pairs:
            result = self.evaluate_single_query(
                qa_pair,
                self.config.retrieval.num_docs,
                prompt_template
            )
            
            results.append(result)
            all_metrics.append(result["metrics"])
        
        # Aggregate metrics
        aggregated_metrics = self._aggregate_metrics(all_metrics)
        
        return {
            "config": self.config.to_dict(),
            "individual_results": results,
            "aggregated_metrics": aggregated_metrics,
            "num_queries": len(results)
        }
    
    def _aggregate_metrics(
        self,
        all_metrics: List[Dict[str, float]]
    ) -> Dict[str, float]:
        """
        Aggregate metrics across all queries
        
        Args:
            all_metrics: List of metric dictionaries
            
        Returns:
            Aggregated metrics
        """
        if not all_metrics:
            return {}
        
        aggregated = {}
        metric_keys = all_metrics[0].keys()
        
        for key in metric_keys:
            values = [m[key] for m in all_metrics if key in m]
            if values:
                aggregated[f"avg_{key}"] = np.mean(values)
                aggregated[f"std_{key}"] = np.std(values)
                aggregated[f"min_{key}"] = np.min(values)
                aggregated[f"max_{key}"] = np.max(values)
        
        return aggregated
    
    def run_with_mlflow(self) -> None:
        """Run evaluation and log to MLflow"""
        run_name = self.config.mlflow.run_name or f"run_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        with mlflow.start_run(run_name=run_name) as run:
            logger.info(f"Started MLflow run: {run.info.run_id}")
            
            # Log parameters
            mlflow.log_params(self.config.to_dict()["retrieval"])
            mlflow.log_params(self.config.to_dict()["model"])
            mlflow.log_params(self.config.to_dict()["prompt"])
            
            # Log tags
            mlflow.set_tags(self.config.mlflow.tags)
            
            # Run evaluation
            results = self.run_evaluation()
            
            # Log metrics
            for metric_name, metric_value in results["aggregated_metrics"].items():
                mlflow.log_metric(metric_name, metric_value)
            
            # Save detailed results
            results_path = Path(f"results_{run.info.run_id}.json")
            with open(results_path, 'w') as f:
                json.dump(results, f, indent=2)
            
            mlflow.log_artifact(str(results_path))
            results_path.unlink()  # Clean up
            
            logger.info(f"Completed MLflow run: {run.info.run_id}")
            logger.info(f"Aggregated metrics: {results['aggregated_metrics']}")


def grid_search(
    base_config: ExperimentConfig,
    param_grid: Dict[str, List[Any]]
) -> None:
    """
    Perform grid search over hyperparameters
    
    Args:
        base_config: Base configuration
        param_grid: Parameter grid for search
    """
    logger.info("Starting grid search...")
    
    # Generate all combinations
    keys = param_grid.keys()
    values = param_grid.values()
    combinations = list(itertools.product(*values))
    
    logger.info(f"Total combinations: {len(combinations)}")
    
    for idx, combination in enumerate(combinations, 1):
        logger.info(f"\n{'='*80}")
        logger.info(f"Combination {idx}/{len(combinations)}")
        logger.info(f"{'='*80}")
        
        # Create config for this combination
        config = ExperimentConfig(
            retrieval=base_config.retrieval,
            model=base_config.model,
            prompt=base_config.prompt,
            paths=base_config.paths,
            mlflow=base_config.mlflow
        )
        
        # Update with grid parameters
        params = dict(zip(keys, combination))
        
        if "num_docs" in params:
            config.retrieval.num_docs = params["num_docs"]
        if "retrieval_method" in params:
            config.retrieval.method = params["retrieval_method"]
        if "prompt_id" in params:
            config.prompt.prompt_id = params["prompt_id"]
        if "temperature" in params:
            config.model.temperature = params["temperature"]
        if "model_name" in params:
            config.model.name = params["model_name"]
        
        # Update run name and tags
        config.mlflow.run_name = f"grid_search_{idx}"
        config.mlflow.tags = {
            "grid_search": "true",
            "combination": str(idx),
            **params
        }
        
        logger.info(f"Parameters: {params}")
        
        try:
            # Run evaluation
            evaluator = RAGEvaluator(config)
            evaluator.load_components()
            evaluator.run_with_mlflow()
            
            # Clear GPU cache
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
        except Exception as e:
            logger.error(f"Error in combination {idx}: {str(e)}")
            continue
